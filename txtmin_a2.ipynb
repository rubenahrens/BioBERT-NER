{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextMining Assignment 2\n",
    "Lucas de Wolff (s3672980) and Ruben Ahrens (s3677532)\n",
    "\n",
    "November 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from huggingface_hub import interpreter_login\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"O\", \"B-corporation\", \"I-corporation\", \"B-creative-work\", \"I-creative-work\", \"B-group\", \n",
    "       \"I-group\", \"B-location\", \"I-location\", \"B-person\", \"I-person\", \"B-product\", \"I-product\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = 'wnut17train.conll', 'emerging.dev.conll', 'emerging.test.annotated'\n",
    "path = 'W-NUT_data'\n",
    "\n",
    "raw_datasets = {}\n",
    "for file, name in zip([train, valid, test], ['train', 'validation', 'test']):\n",
    "    id = 0\n",
    "    raw_datasets[name] = {'id': [], 'tokens': [], 'ner_tags': []}\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                token, ner_tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(names.index(ner_tag))\n",
    "            except:\n",
    "                raw_datasets[name]['id'].append([id for _ in range(len(tokens))])\n",
    "                raw_datasets[name]['tokens'].append(tokens)\n",
    "                raw_datasets[name]['ner_tags'].append(ner_tags)\n",
    "                id += 1\n",
    "                tokens, ner_tags = [], []\n",
    "    raw_datasets[name] = datasets.Dataset.from_dict(raw_datasets[name])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "display(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff728bdfbf2140c5b79378a189360179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9100000591364574b63cc4a466b95813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e0f7310bec45f0b9079ba27f2eee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14878 4102 6348\n",
      "11810 1847 3776\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# compare total number of tokens before and after tokenization. which is not the number of rows, but the number of tokens\n",
    "train_set = set([word  for sentence  in  raw_datasets['train']['tokens'] for word in sentence ])\n",
    "eval_set = set([word  for  sentence  in raw_datasets['validation']['tokens'] for word in sentence ])\n",
    "test_set = set([word  for  sentence  in raw_datasets['test']['tokens'] for word in sentence ])\n",
    "\n",
    "print(len(train_set), len(eval_set), len(test_set))\n",
    "\n",
    "# Print unique tokens of each dataset\n",
    "train_set_unique = train_set - eval_set - test_set\n",
    "eval_set_unique = eval_set - train_set - test_set\n",
    "test_set_unique = test_set - train_set - eval_set\n",
    "\n",
    "print(len(train_set_unique), len(eval_set_unique), len(test_set_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tex_table(all_metrics, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"\\\\begin{table}[h]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lrrrr}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\" & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1} & \\\\textbf{Number} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        for key, value in all_metrics.items():\n",
    "            if \"overall\" not in key:\n",
    "                f.write(f\"{key.title()} & {value['precision']:.2f} & {value['recall']:.2f} & {value['f1']:.2f} & {value['number']} \\\\\\\\\\n\")\n",
    "        try:\n",
    "            f.write(\"\\\\midrule\\n\")\n",
    "            f.write(\" & \\\\textbf{Overall precision} & \\\\textbf{Overall recall} & \\\\textbf{Overall F1} & \\\\textbf{Overall accuracy} \\\\\\\\\\n\")\n",
    "            f.write(\"\\\\midrule\\n\")\n",
    "            f.write(f\" & {all_metrics['overall_precision']:.2f} & {all_metrics['overall_recall']:.2f} & {all_metrics['overall_f1']:.2f} & {all_metrics['overall_accuracy']:.2f} \\\\\\\\\\n\")\n",
    "        except:\n",
    "            pass\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "def print_fancy_table(all_metrics):\n",
    "    # display fancy table in ipython widget\n",
    "    from IPython.display import display, HTML\n",
    "    html = \"<table>\"\n",
    "    html += \"<tr><th></th><th>Precision</th><th>Recall</th><th>F1</th><th>Number</th></tr>\"\n",
    "    for key, value in all_metrics.items():\n",
    "        if \"overall\" not in key:\n",
    "            html += f\"<tr><td>{key.title()}</td><td>{value['precision']:.2f}</td><td>{value['recall']:.2f}</td><td>{value['f1']:.2f}</td><td>{value['number']}</td></tr>\"\n",
    "    html += \"<tr><th></th><th>Overall precision</th><th>Overall recall</th><th>Overall F1</th><th>Overall accuracy</th></tr>\"\n",
    "    html += f\"<tr><td></td><td>{all_metrics['overall_precision']:.2f}</td><td>{all_metrics['overall_recall']:.2f}</td><td>{all_metrics['overall_f1']:.2f}</td><td>{all_metrics['overall_accuracy']:.2f}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Ruben\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6b17ce41e4a9bae10ffe536729eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad91114bb9d44d42ac1ff99fd76a4574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34034600853919983, 'eval_precision': 0.5636672325976231, 'eval_recall': 0.39712918660287083, 'eval_f1': 0.4659649122807018, 'eval_accuracy': 0.9172063152261172, 'eval_runtime': 3.5193, 'eval_samples_per_second': 286.704, 'eval_steps_per_second': 36.087, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26775ecf2c54caa8e681557f9354ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3707972764968872, 'eval_precision': 0.6083916083916084, 'eval_recall': 0.41626794258373206, 'eval_f1': 0.4943181818181819, 'eval_accuracy': 0.9217018999197217, 'eval_runtime': 3.487, 'eval_samples_per_second': 289.361, 'eval_steps_per_second': 36.421, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151e9e4ecd5b4e07b310c29ce55ca9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3559342920780182, 'eval_precision': 0.5850439882697948, 'eval_recall': 0.4772727272727273, 'eval_f1': 0.525691699604743, 'eval_accuracy': 0.9261439657479261, 'eval_runtime': 3.4459, 'eval_samples_per_second': 292.811, 'eval_steps_per_second': 36.855, 'epoch': 3.0}\n",
      "{'train_runtime': 172.2891, 'train_samples_per_second': 59.098, 'train_steps_per_second': 7.4, 'train_loss': 0.1302520751953125, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bb487c2652462f883c78b6e12ee3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9926be48e17b4e92959d3735d4fc1598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426131bdc42247758013ad31c085c663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36d47ee636f4942b28e0cc97d4039ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th></th><th>Precision</th><th>Recall</th><th>F1</th><th>Number</th></tr><tr><td>Corporation</td><td>0.19</td><td>0.18</td><td>0.19</td><td>66</td></tr><tr><td>Creative-Work</td><td>0.41</td><td>0.20</td><td>0.27</td><td>142</td></tr><tr><td>Group</td><td>0.35</td><td>0.13</td><td>0.19</td><td>165</td></tr><tr><td>Location</td><td>0.55</td><td>0.46</td><td>0.50</td><td>150</td></tr><tr><td>Person</td><td>0.75</td><td>0.45</td><td>0.57</td><td>429</td></tr><tr><td>Product</td><td>0.15</td><td>0.07</td><td>0.10</td><td>127</td></tr><tr><th></th><th>Overall precision</th><th>Overall recall</th><th>Overall F1</th><th>Overall accuracy</th></tr><tr><td></td><td>0.52</td><td>0.31</td><td>0.39</td><td>0.94</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids\n",
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "write_tex_table(all_metrics, 'bert-finetuned-ner_baseline.txt')\n",
    "print_fancy_table(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2023-11-07 23:44:37,240] A new study created in memory with name: no-name-16a160b2-5ef5-4a9f-b970-b6fc0bcda730\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a6e571da7463c8ffe2c5b268894db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7cff0df5534759a5cdb18b7a95b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36082062125205994, 'eval_precision': 0.534923339011925, 'eval_recall': 0.37559808612440193, 'eval_f1': 0.4413211524947294, 'eval_accuracy': 0.914905004013915, 'eval_runtime': 3.6235, 'eval_samples_per_second': 278.459, 'eval_steps_per_second': 35.049, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43066123d176485a9fd98e830024f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31303343176841736, 'eval_precision': 0.5113314447592068, 'eval_recall': 0.4318181818181818, 'eval_f1': 0.46822308690012965, 'eval_accuracy': 0.9232004281509232, 'eval_runtime': 3.6412, 'eval_samples_per_second': 277.104, 'eval_steps_per_second': 34.878, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8715347263d4945b88442efa1accc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-07 23:46:31,036] Trial 0 finished with value: 2.4381972240284178 and parameters: {'learning_rate': 7.46444021494764e-05, 'per_device_train_batch_size': 32, 'beta_1': 0.9823212753255092, 'beta_2': 0.9561558831152476, 'weight_decay': 0.0572085027093759, 'adam_epsilon': 7.561691368560218e-09}. Best is trial 0 with value: 2.4381972240284178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3379002809524536, 'eval_precision': 0.5438108484005564, 'eval_recall': 0.4677033492822967, 'eval_f1': 0.5028938906752413, 'eval_accuracy': 0.9237891356703238, 'eval_runtime': 3.5239, 'eval_samples_per_second': 286.331, 'eval_steps_per_second': 36.04, 'epoch': 3.0}\n",
      "{'train_runtime': 112.7447, 'train_samples_per_second': 90.31, 'train_steps_per_second': 2.847, 'train_loss': 0.14821997906931464, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2ae4382f734e4fb6a0158a9bcabc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18f19e82a5a49169b0e20317004ccf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ruben\\miniconda3\\envs\\torch\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43544960021972656, 'eval_precision': 0.436241610738255, 'eval_recall': 0.07775119617224881, 'eval_f1': 0.1319796954314721, 'eval_accuracy': 0.8906074391222906, 'eval_runtime': 3.6181, 'eval_samples_per_second': 278.876, 'eval_steps_per_second': 35.101, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12630e85caa42ca81bb63ec68b6fbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3932664394378662, 'eval_precision': 0.589098532494759, 'eval_recall': 0.3361244019138756, 'eval_f1': 0.4280274181264281, 'eval_accuracy': 0.9105164570511105, 'eval_runtime': 3.5064, 'eval_samples_per_second': 287.759, 'eval_steps_per_second': 36.219, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad826eeda74479aad8047fd97bf2983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-07 23:48:47,536] Trial 1 finished with value: 2.2580753449113597 and parameters: {'learning_rate': 9.97282119646338e-06, 'per_device_train_batch_size': 16, 'beta_1': 0.920206368594119, 'beta_2': 0.9894853335051167, 'weight_decay': 0.05079995961253338, 'adam_epsilon': 2.4714995116229446e-08}. Best is trial 1 with value: 2.2580753449113597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37271803617477417, 'eval_precision': 0.5221674876847291, 'eval_recall': 0.3803827751196172, 'eval_f1': 0.4401384083044983, 'eval_accuracy': 0.9153866738025154, 'eval_runtime': 3.6117, 'eval_samples_per_second': 279.369, 'eval_steps_per_second': 35.163, 'epoch': 3.0}\n",
      "{'train_runtime': 135.3304, 'train_samples_per_second': 75.238, 'train_steps_per_second': 4.722, 'train_loss': 0.20646982685501028, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned-ner-optuna\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "def model_init(): \n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")    \n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"adam_beta1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        \"adam_beta2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 9.97282119646338e-06\n",
      "per_device_train_batch_size: 16\n",
      "beta_1: 0.920206368594119\n",
      "beta_2: 0.9894853335051167\n",
      "weight_decay: 0.05079995961253338\n",
      "adam_epsilon: 2.4714995116229446e-08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa982b754b314816bc3a4ab94eab2d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf68c78522543afb467af8f2ca74502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3317992091178894, 'eval_precision': 0.5573549257759784, 'eval_recall': 0.49401913875598086, 'eval_f1': 0.5237793278376665, 'eval_accuracy': 0.9252876639015253, 'eval_runtime': 3.5636, 'eval_samples_per_second': 283.14, 'eval_steps_per_second': 35.638, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1f36a79101491d867a8cfefec57be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38918423652648926, 'eval_precision': 0.5584958217270195, 'eval_recall': 0.4796650717703349, 'eval_f1': 0.5160875160875161, 'eval_accuracy': 0.9249130318437249, 'eval_runtime': 3.6303, 'eval_samples_per_second': 277.938, 'eval_steps_per_second': 34.983, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663b239af02c4507aa0e494141049192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3891257345676422, 'eval_precision': 0.5642076502732241, 'eval_recall': 0.49401913875598086, 'eval_f1': 0.5267857142857143, 'eval_accuracy': 0.9265185978057265, 'eval_runtime': 3.4944, 'eval_samples_per_second': 288.747, 'eval_steps_per_second': 36.344, 'epoch': 3.0}\n",
      "{'train_runtime': 136.7229, 'train_samples_per_second': 74.472, 'train_steps_per_second': 4.674, 'train_loss': 0.03902594696188197, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4880fa76267043f5bd2d80dadf77d0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd8c84373c849aab994dfb7618cdc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc69d95cdd54ce19263829ac448f7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/rubinho/bert-finetuned-ner-optuna/tree/main/'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    print(f\"{n}: {v}\")\n",
    "    setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0f5deb610a4a9cb4ea458b34263d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  corporation       0.19      0.21      0.20        66\n",
      "creative-work       0.44      0.22      0.29       142\n",
      "        group       0.45      0.18      0.25       165\n",
      "     location       0.54      0.41      0.47       150\n",
      "       person       0.75      0.46      0.57       429\n",
      "      product       0.16      0.09      0.12       127\n",
      "\n",
      "    micro avg       0.52      0.32      0.40      1079\n",
      "    macro avg       0.42      0.26      0.32      1079\n",
      " weighted avg       0.53      0.32      0.40      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "logits, labels = eval_preds\n",
    "predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Remove ignored index (special tokens) and convert to labels\n",
    "true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "true_predictions = [\n",
    "    [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-corporation</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.24</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-corporation</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-creative-work</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.38</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-creative-work</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-group</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.29</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-group</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-location</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-location</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.47</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-person</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.61</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-person</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.47</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-product</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-product</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 precision  recall    f1  number\n",
       "B-corporation         0.27    0.21  0.24      66\n",
       "I-corporation         0.31    0.21  0.25     133\n",
       "B-creative-work       0.68    0.27  0.38     142\n",
       "I-creative-work       0.75    0.20  0.31     442\n",
       "B-group               0.58    0.19  0.29     165\n",
       "I-group               0.45    0.14  0.21     242\n",
       "B-location            0.66    0.47  0.55     150\n",
       "I-location            0.74    0.35  0.47     237\n",
       "B-person              0.82    0.49  0.61     429\n",
       "I-person              0.83    0.32  0.47     918\n",
       "B-product             0.48    0.18  0.26     127\n",
       "I-product             0.62    0.28  0.39     359"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flatten the labels and predictions\n",
    "true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n",
    "\n",
    "# compute precision, recall, and f1 per entity type\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average=None, labels=names[1:])\n",
    "\n",
    "# count number of entities per type\n",
    "from collections import Counter\n",
    "count = Counter(true_labels_flat)\n",
    "\n",
    "# format into pandas table and round to 2 decimals\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'precision': precision, 'recall': recall, 'f1': f1, 'number': count}, index=names[1:])\n",
    "df = df.round(2)\n",
    "display(df)\n",
    "write_tex_table(df.to_dict('index'), 'bert-finetuned-ner_optuna.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
