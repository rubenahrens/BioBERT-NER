{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextMining Final Project (BioBert)\n",
    "Lucas de Wolff (s3672980) and Ruben Ahrens (s3677532)\n",
    "\n",
    "January 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from huggingface_hub import interpreter_login\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from helper import write_tex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['O','B-adr', 'B-disease', 'B-drug', 'B-symptom', 'B-finding','I-adr', 'I-disease', 'I-drug', 'I-symptom', 'I-finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 988\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 185\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 132\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = 'train.ann', 'valid.ann', 'test.ann'\n",
    "path = '../../cadec/processed/'\n",
    "\n",
    "raw_datasets = {}\n",
    "for file, name in zip([train, valid, test], ['train', 'validation', 'test']):\n",
    "    id = 0\n",
    "    raw_datasets[name] = {'id': [], 'tokens': [], 'ner_tags': []}\n",
    "    with open(f'{path}/{file}', 'r') as f:\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            try:\n",
    "                token, ner_tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(names.index(ner_tag))\n",
    "            except:\n",
    "                raw_datasets[name]['id'].append([id for _ in range(len(tokens))])\n",
    "                raw_datasets[name]['tokens'].append(tokens)\n",
    "                raw_datasets[name]['ner_tags'].append(ner_tags)\n",
    "                id += 1\n",
    "                tokens, ner_tags = [], []\n",
    "    raw_datasets[name] = datasets.Dataset.from_dict(raw_datasets[name])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict(raw_datasets)\n",
    "display(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 1, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 988/988 [00:00<00:00, 2543.63 examples/s]\n",
      "Map: 100%|██████████| 185/185 [00:00<00:00, 3243.88 examples/s]\n",
      "Map: 100%|██████████| 132/132 [00:00<00:00, 2475.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5629 2247 2240\n",
      "2983 271 270\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# compare total number of tokens before and after tokenization. which is not the number of rows, but the number of tokens\n",
    "train_set = set([word  for sentence  in  raw_datasets['train']['tokens'] for word in sentence ])\n",
    "eval_set = set([word  for  sentence  in raw_datasets['validation']['tokens'] for word in sentence ])\n",
    "test_set = set([word  for  sentence  in raw_datasets['test']['tokens'] for word in sentence ])\n",
    "\n",
    "print(len(train_set), len(eval_set), len(test_set))\n",
    "\n",
    "# Print unique tokens of each dataset\n",
    "train_set_unique = train_set - eval_set - test_set\n",
    "eval_set_unique = eval_set - train_set - test_set\n",
    "test_set_unique = test_set - train_set - eval_set\n",
    "\n",
    "print(len(train_set_unique), len(eval_set_unique), len(test_set_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dmis-lab/biobert-v1.1\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\lcdew\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcdew\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:351: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-03 23:22:25,361]\u001b[0m A new study created in memory with name: no-name-8dd29556-3926-401f-b100-834d0adb1840\u001b[0m\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\lcdew\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█</td></tr><tr><td>eval/f1</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/precision</td><td>▁█</td></tr><tr><td>eval/recall</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁▁</td></tr><tr><td>train/train_loss</td><td>█▁</td></tr><tr><td>train/train_runtime</td><td>▁█</td></tr><tr><td>train/train_samples_per_second</td><td>█▁</td></tr><tr><td>train/train_steps_per_second</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88871</td></tr><tr><td>eval/f1</td><td>0.45396</td></tr><tr><td>eval/loss</td><td>0.35087</td></tr><tr><td>eval/precision</td><td>0.46261</td></tr><tr><td>eval/recall</td><td>0.44562</td></tr><tr><td>eval/runtime</td><td>6.945</td></tr><tr><td>eval/samples_per_second</td><td>26.638</td></tr><tr><td>eval/steps_per_second</td><td>3.456</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>124</td></tr><tr><td>train/total_flos</td><td>153634579142064.0</td></tr><tr><td>train/train_loss</td><td>0.34114</td></tr><tr><td>train/train_runtime</td><td>386.5116</td></tr><tr><td>train/train_samples_per_second</td><td>2.556</td></tr><tr><td>train/train_steps_per_second</td><td>0.321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-paper-1</strong> at: <a href=\"https://wandb.ai/team_tien/huggingface/runs/zhntwerz\" target=\"_blank\">https://wandb.ai/team_tien/huggingface/runs/zhntwerz</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_223020-zhntwerz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlcdewolff\u001b[0m (\u001b[33mteam_tien\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826685ee91d44096b0f3d3c9ad3bd382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lcdew\\Desktop\\Onedrive-Folder\\CS - AI\\Text Mining\\textmining-project\\Code\\NER\\wandb\\run-20240103_232232-i0x7ex1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/team_tien/huggingface/runs/i0x7ex1h\" target=\"_blank\">fluent-spaceship-2</a></strong> to <a href=\"https://wandb.ai/team_tien/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/team_tien/huggingface\" target=\"_blank\">https://wandb.ai/team_tien/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/team_tien/huggingface/runs/i0x7ex1h\" target=\"_blank\">https://wandb.ai/team_tien/huggingface/runs/i0x7ex1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [09:28<00:00,  3.05s/it]c:\\Users\\lcdew\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "100%|██████████| 124/124 [09:48<00:00,  4.74s/it]\n",
      "\u001b[32m[I 2024-01-03 23:32:22,440]\u001b[0m Trial 0 finished with value: 1.7191650040620825 and parameters: {'learning_rate': 1.7048843041374682e-06, 'per_device_train_batch_size': 8, 'beta_1': 0.9647317246381171, 'beta_2': 0.9020145905293318, 'weight_decay': 0.06361581414539838, 'adam_epsilon': 2.6408216627517734e-09}. Best is trial 0 with value: 1.7191650040620825.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3835258483886719, 'eval_precision': 1.0, 'eval_recall': 0.0007062146892655367, 'eval_f1': 0.0014114326040931544, 'eval_accuracy': 0.717047356768724, 'eval_runtime': 20.1198, 'eval_samples_per_second': 9.195, 'eval_steps_per_second': 1.193, 'epoch': 1.0}\n",
      "{'train_runtime': 595.244, 'train_samples_per_second': 1.66, 'train_steps_per_second': 0.208, 'train_loss': 1.6337023089008946, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"biobert-cadec-ner-optuna\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "def model_init(): \n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"adam_beta1\": trial.suggest_float(\"beta_1\", 0.9, 0.999),\n",
    "        \"adam_beta2\": trial.suggest_float(\"beta_2\", 0.9, 0.999),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Results/biobert/best_run_biobert_cadec_ner_optuna.txt', 'wb') as f:\n",
    "    for key, value in best_run.hyperparameters.items():\n",
    "        f.write(f'{key}: {value}\\n'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"biobert-cadec-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=50,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "# Load from text file\n",
    "# with open('best_run_biobert_cadec_ner_optuna.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         n, v = line.split(': ')\n",
    "#         setattr(trainer.args, n, v)\n",
    "    \n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "eval_preds = predictions.predictions, predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         adr       0.00      0.00      0.00       800\n",
      "     disease       1.00      0.01      0.01       171\n",
      "        drug       0.00      0.00      0.00       217\n",
      "     finding       0.00      0.00      0.00        60\n",
      "     symptom       0.00      0.00      0.00        80\n",
      "\n",
      "   micro avg       1.00      0.00      0.00      1328\n",
      "   macro avg       0.20      0.00      0.00      1328\n",
      "weighted avg       0.13      0.00      0.00      1328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_metrics = compute_all_metrics(eval_preds)\n",
    "logits, labels = eval_preds\n",
    "predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Remove ignored index (special tokens) and convert to labels\n",
    "true_labels = [[names[l] for l in label if l != -100] for label in labels]\n",
    "true_predictions = [\n",
    "    [names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the labels and predictions\n",
    "true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n",
    "\n",
    "# compute precision, recall, and f1 per entity type\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, true_predictions_flat, average=None, labels=names[1:])\n",
    "\n",
    "# count number of entities per type\n",
    "from collections import Counter\n",
    "count = Counter(true_labels_flat)\n",
    "\n",
    "# format into pandas table and round to 2 decimals\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'precision': precision, 'recall': recall, 'f1': f1, 'number': count}, index=names[1:])\n",
    "df = df.round(2)\n",
    "display(df)\n",
    "write_tex_table(df.to_dict('index'), 'Results/BioBert/biobert-finetuned-evaluationmetrics.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
